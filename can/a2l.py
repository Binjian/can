# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01.a2l.ipynb.

# %% auto 0
__all__ = ['parser', 'args', 'type_collection', 'A2LType', 'list_of_strings', 'JsonNodePathSegment', 'JsonNodePath',
           'get_argparser', 'Bunch', 'Record', 'Calibration', 'Measurement', 'AxisScale', 'DataConversion',
           'DataLayout', 'load_class_type_a2l_lazy', 'load_records_lazy', 'XCPConfig', 'check_a2l_type', 'XCPData',
           'Get_Init_XCPData', 'XCPCalib', 'Get_XCPCalib_From_XCP', 'Generate_XCPData', 'load_a2l_lazy',
           'load_a2l_eager']

# %% ../nbs/01.a2l.ipynb 5
import ijson
import json
import inspect
from typing import Optional, Union, List
from functools import cached_property, cache
import re
from enum import Enum
from pathlib import Path
from pprint import pprint, pformat
import argparse
from InquirerPy import inquirer
from InquirerPy.validator import NumberValidator, EmptyInputValidator, PathValidator
from InquirerPy.base.control import Choice
from pydantic import BaseModel, Field, validator, field_validator, model_validator, conlist, model_serializer, ValidationError
from pydantic.functional_validators import AfterValidator
from typing_extensions import Annotated, TypeAliasType

# %% ../nbs/01.a2l.ipynb 6
def list_of_strings(strings: str)->list[str]:
	""" split a string separated by ',', ';', or '\s' to a list of strings.
	Descripttion: split a long string to a list of strings.

	Args:
		strings (str): The string to split.

	Returns:
		list: The list of strings.
	"""
	return re.split(r',\s*|;\s*|\s+', strings)	


# %% ../nbs/01.a2l.ipynb 8
class JsonNodePathSegment:
	"""result of parsing json node path segment
	
	Args:
		name (str): name of the node
		indices (list[int]): indices of the node
		index_range (list[int]): index range of the node

		if both indices and index_range are None, then the node is a dict, otherwise it is a list

	"""
	def __init__(self, name: str, indices: list[int]=None, index_range: list[int]=None):
		self.name = name
		self.indices = indices
		self.index_range = index_range
		assert (
			(indices is None and index_range is None)
			or (indices is [] and index_range is None)  # empty list is valid for lazy loading
			or (indices is not None and index_range is None) 
			or (indices is None and index_range is not None)
		), 'Invalid JsonNodePathSegment'

	def __repr__(self):
		if self.indices is None and self.index_range is None:
			return f'<{self.name} dict>'
		elif self.indices == []:
			return f'<{self.name}[] list>'
		elif self.indices is not None:
			return f'<{self.name}[{",".join([str(i) for i in self.indices])}] list>'
		elif len(self.index_range)==2:  #self.index_range is not None:
			return f'<{self.name}[{self.index_range[0]}:{self.index_range[1]}] list>'
		elif len(self.index_range)==3:  #self.index_range is not None:
			return f'<{self.name}[{self.index_range[0]}:{self.index_range[1]}:{self.index_range[2]}] list>'
		else:
			raise ValueError(f'Invalid index range {self.index_range}')

	@property	
	def is_dict(self):
		return self.indices is None and self.index_range is None

# %% ../nbs/01.a2l.ipynb 9
class JsonNodePath:
	"""result of parsing json node path
	
	Args:
		segments (list[JsonNodePathSegment]): list of JsonNodePathSegment

	"""
	def __init__(self, node_path: str):
		""" Parse the json data to get the node specified by the node path.
		Descripttion: Parse the json data to get the node specified by the node path.

		Args:
			node_path (str): The node path to the node.
			json_data (dict): The json data to parse.

		Returns:
			dict: The node specified by the node path.
		"""
		self.node_path_str = node_path
		path_segments = re.split(r'/\s*', node_path)[1:]
		self.node_path_segments = []
		for s in path_segments:
			name = re.search(r'(\w+)', s).group(1)
			# res = re.search('(?:\[(\d+)((?:(,?\s*(\d*))*)|(?:(:?\s*(\d*))*))\])', s)  # regex with \d+ for mandatory digit in [] pair
			res = re.search('(?:\[(\d*)((?:(,?\s*(\d*))*)|(?:(:?\s*(\d*))*))\])', s)  # regex with \d* for optional empyt [] pair
			if res:
				r = res.groups()
				if r[0]=='' and r[1]=='':
					# print(f'{name} is a list : index: {r[0]}')
					self.node_path_segments.append(JsonNodePathSegment(name=name,indices=[]))  # append empty list for lazy finding and loading
				elif r[0]!='' and r[1]=='':
					# print(f'{name} is a list : index: {r[0]}')
					self.node_path_segments.append(JsonNodePathSegment(name=name,indices=[int(r[0])]))
				elif ',' in r[1]:
					indices = re.split(r',\s*', r[1])
					indices[0] = r[0]
					# print(f'{name} is a list : indices {indices}')
					self.node_path_segments.append(JsonNodePathSegment(name=name,indices=indices))
				elif ':' in r[1]:
					index_range = re.split(r':\s*', r[1])
					index_range[0] = r[0]
					# if len(index_range)==2:
					# 	print(f'{name} is a list : index range [{index_range[0]}:{index_range[1]}]')
					# elif len(index_range)==3:
					# 	print(f'{name} is a list : index range [{index_range[0]}:{index_range[1]}:{index_range[2]}]')
					# else:
					# 	raise ValueError(f'Invalid index range {index_range}')
					self.node_path_segments.append(JsonNodePathSegment(name=name,index_range=index_range))
				else:
					raise ValueError(f'Invalid index spec in {s}')
			else:
				# print(f'{name} is a dict')
				self.node_path_segments.append(JsonNodePathSegment(name=name))
	def __repr__(self):
		return f'<JsonNodePath {self.node_path_segments}>'

	def __iter__(self):
		return (s for s in self.node_path_segments)	

	@property
	def leaf(self):
		"""return the leaf of the node path"""
		return self.node_path_segments[-1]

	@property
	def lazy_path(self):
		"""return the lazy path of the node path"""
		return '.'.join([f'{s.name}' if s.is_dict else f'{s.name}.item' for s in self.node_path_segments])

# %% ../nbs/01.a2l.ipynb 11
def get_argparser()->argparse.ArgumentParser:
	""" Get the argument parser for the command line interface.
	Descripttion: Get the argument parser for the command line interface.

	Returns:
		argparse.ArgumentParser: The argument parser for the command line interface.
	"""
	import re

	parser = argparse.ArgumentParser(
        "Get the A2L file path and the desired configuration for CCP/XCP.",
    )

	parser.add_argument(
        "-p",
        "--path",
        type=str,
        default=r"../res/vbu_sample.json",
        help="path to the A2L file",
    )

	parser.add_argument(
		"-n",
		"--node-path",
		type=JsonNodePath,
		default=r"/PROJECT/MODULE[]",
		# default=r"/PROJECT/MODULE[]/CHARACTERISTIC[], "
		# 		r"/PROJECT/MODULE[]/MEASUREMENT[], "
		# 		r"/PROJECT/MODULE[]/AXIS_PTS[], "
		# 		r"/PROJECT/MODULE[]/COMPU_METHOD[], ",
		help="node path to search for calibration parameters",
	)

	parser.add_argument(
		"-l",
		"--leaves",
		type=list_of_strings,
		default=r"TQD_trqTrqSetNormal_MAP_v, " 
				r"VBU_L045A_CWP_05_09T_AImode_CM_single, " 
				r"Lookup2D_FLOAT32_IEEE, " 
				r"Lookup2D_X_FLOAT32_IEEE, " 
				r"Scalar_FLOAT32_IEEE, " 
				r"TQD_vVehSpd, "
				r"TQD_vSgndSpd_MAP_y, "
				r"TQD_pctAccPedPosFlt, "
				r"TQD_pctAccPdl_MAP_x",
			help="leaf nodes to search for",
	)

	return parser

# %% ../nbs/01.a2l.ipynb 12
parser = get_argparser()
args = parser.parse_args(
	[
		"-p",
		# r"../res/VBU_AI.json",
		r"../res/vbu_sample.json",
		"-n",
		r"/PROJECT/MODULE[], ",
		# r"/PROJECT/MODULE[]/CHARACTERISTIC[], "
		# 	r"/PROJECT/MODULE[]/MEASUREMENT[], "
		# 	r"/PROJECT/MODULE[]/AXIS_PTS[], "
		# 	r"/PROJECT/MODULE[]/COMPU_METHOD[]",
		"-l",
		r"TQD_trqTrqSetNormal_MAP_v, " 
				r"VBU_L045A_CWP_05_09T_AImode_CM_single, " 
				r"Lookup2D_FLOAT32_IEEE, " 
				r"Lookup2D_X_FLOAT32_IEEE, " 
				r"Scalar_FLOAT32_IEEE, " 
				r"TQD_vVehSpd, "
				r"TQD_vSgndSpd_MAP_y, "
				r"TQD_pctAccPedPosFlt, "
				r"TQD_pctAccPdl_MAP_x",
	]
)
args.__dict__

# %% ../nbs/01.a2l.ipynb 23
class Bunch(object):
	"""collector of a bunch of named stuff into one object; a generic record/struct type, indexed by keys"""
	bunch_registry = {} 
	def __init__(self, key, **kwargs):
		"""Bunch object self contains no 'key' attribute, but adict could have."""
		self.key = key
		self.__dict__.update(kwargs)
		self.__class__.bunch_registry.update({key: self})
	
	def __repr__(self):
		return f'<{self.__class__.__name__}.{self.key}>'
	
	def __hash__(self) -> int:
		return hash(tuple(sorted(self.__dict__.items())))
	
	def __eq__(self, other) -> bool: 
		return self.__dict__ == other.__dict__
	
	@staticmethod
	def fetch(key):
		return Bunch.__index[key]
	
	# @classmethod
	# def register(cls, key: str, value: bunch):
	# 	"""manual registration of a bunch object with a key
		
	# 	args:
	# 		key: jnode_path string
	# 		value: bunch object
			
	# 	"""
	# 	cls.bunch_registry.update({key: value})
	

# %% ../nbs/01.a2l.ipynb 24
class Record:
	"""object with dynamic attributes"""
	record_registry = None
	__RecordCats = None
	__cat = None 
	subclass_registry = {}

	def __init__(self, **kwargs):
		self.__dict__.update(kwargs)

	def __repr__(self):
		return f'<{self.__class__.__name__}: {self.Name!r}>'

	@staticmethod
	def fetch(key: str) -> Union[Record, str]:
		try:
			rec = Record.record_registry[key]
		except KeyError:
			rec = key.split('.')[-1]

		return rec 
	
	@classmethod
	def load_types(cls, path: Path, jnode_path: Optional[JsonNodePath] = JsonNodePath('/PROJECT/MODULE[]')) -> None:
		"""
		Load types for the Record class.

		Args:
			path (Path): The path to the file.
			jnode_path (Optional[JsonNodePath], optional): The JSON node path. Defaults to JsonNodePath('/PROJECT/MODULE[]').
		"""
		cls.__RecordCats = load_class_type_a2l_lazy(path, jnode_path)

	@classmethod
	def load_records(cls, path: Path, keys: list[str], jnode_path: Optional[JsonNodePath] = JsonNodePath('/PROJECT/MODULE[]')) -> None:
		"""
		Load records for the Record class.

		Args:
			path (Path): The path to the file.
			keys (list[str]): The list of keys.
			jnode_path (Optional[JsonNodePath], optional): The JSON node path. Defaults to JsonNodePath('/PROJECT/MODULE[]').
		"""
		cls.load_types(path, jnode_path)
		cls.record_registry = load_records_lazy(path, keys, jnode_path)

# %% ../nbs/01.a2l.ipynb 25
class Calibration(Record):
	"""Target calibration object for torque map; a2l section ["PROJECT"]["MODULE"]["CHARACTERISTIC"]
	
	First level keys will be turned into attributes of the object, encoded registered values will be replaced with the corresponding objects.
	Otherwiese the key-value pairs will be kept as is. 
	""" 
	__cat = 'CHARACTERISTIC'
	Record.subclass_registry[__cat] = 'Calibration'

	def __init__(self, **kwargs):
		# update the dict with the kwargs
		super().__init__(**kwargs)
		# TODO (optionally) add check for the class existence of the keys with ijson parser event handler


	def __repr__(self):
		try:
			return f'<{self.__class__.__name__}: {self.Name!r}>'
		except AttributeError:
			return super().__repr__() 

	@cached_property
	def data_conversion(self):
		try:
			key = self.__dict__['Conversion']['Value']  # define the key for the axis for future fetch
		except KeyError:
			raise KeyError(f'The key "Conversion" is not found in the calibration description or '
						f'the key "Value" is not found in the "Conversion" dict.')
		cat = 'COMPU_METHOD'
		key = f"{super().subclass_registry[cat]}.{key}"  # define the key for the axis for future fetch
		return self.__class__.fetch(key)

	@cached_property
	def record_type(self):
		try:
			key = self.__dict__['Deposit']['Value']  # define the key for the axis for future fetch
		except KeyError:
			raise KeyError(f'The key "Deposit" is not found in the calibration description or '
						f'the key "Value" is not found in the "Deposit" dict.')
		cat = 'RECORD_LAYOUT'
		key = f"{super().subclass_registry[cat]}.{key}"  # define the key for the axis for future fetch
		rtype = self.__class__.fetch(key)
		if type(rtype) is str:  # if the key is not found in the registry, then it is a scalar
			key = f"{super().subclass_registry[cat]}.{'Scalar_' + rtype}"  # construct the new key for the scalar as defined in a2l
			rtype = self.__class__.fetch(key)
		
		return rtype

	@cached_property
	def address(self):
		return hex(int(self.__dict__['Address']['Value']))[2:]   # transform Ecu address to hex string without '0x'

	@cached_property
	def axes(self):  # y axis
		"""_summary_

			for axes[0/1]['InputQuantity']['Value] == 'TQD_vVehSpd' or 'TQD_pctAccPedPosFlt':
		Raises:
			ValueError: _description_
			KeyError: _description_
			KeyError: _description_
			KeyError: _description_

		Returns:
			_type_: _description_
		"""
		try:
			axes = []
			for axis in self.__dict__['AXIS_DESCR']: # 'AXIS_DESCR' is a list of dicts 2 axes for 2D map 
				try:
					if axis['Attribute'] != 'COM_AXIS':
						raise ValueError(f'The value of "Attribute" {axis["Attribute"]} is not "COM_AXIS".')
				except KeyError:
					raise KeyError('The key "Attribute" is not found in the axis description.')
				try:
					key = axis['InputQuantity']['Value']  # define the key for the axis for future fetch
				except KeyError:
					raise KeyError(f'The key "InputQuantity" is not found in the axis description or '
								f'the key "Value" is not found in the "InputQuantity" dict.')
				# get the MEASUREMENT object from the registry				
				cat = 'MEASUREMENT'
				key = f"{super().subclass_registry[cat]}.{key}"
				axis['measurement'] = self.__class__.fetch(key) # replace value with the object

				try:
					key = axis['AXIS_PTS_REF']['AxisPoints']['Value']  # define the key for the axis for future fetch
				except KeyError:
					raise KeyError(f'The key "AXIS_PTS_REF " is not found in the axis description or '
								f'the key "AxisPoins" is not found in the "AXIS_PTS_REF" dict.'
								f'the key "Value" is not found in the "AxisPoints" dict.')
				# get the AXIS_PTS object from the registry				
				cat = 'AXIS_PTS'
				key = f"{super().subclass_registry[cat]}.{key}"
				axis['axis_scale']= self.__class__.fetch(key)  # replace value with object 

				try:
					key = axis['Conversion']['Value']  # define the key for the axis for future fetch
				except KeyError:
					raise KeyError(f'The key "Conversion" is not found in the axis description or '
								f'the key "Value" is not found in the "Conversion" dict.')
				# get the AXIS_PTS object from the registry				
				cat = 'COMPU_METHOD'
				key = f"{super().subclass_registry[cat]}.{key}"
				axis['data_conversion'] = self.__class__.fetch(key)  # replace value with object

				bunch_register_key = f'{self.__class__.__name__}.{self.Name}.{axis["InputQuantity"]["Value"]}' 
				bunch = Bunch(bunch_register_key,**axis)
				axes.append(bunch)
				# Bunch.register(bunch_register_key,bunch)
			
			return axes
		except KeyError:
			raise KeyError('The key "AXIS_DESCR" is not found in the calibration object.')

# %% ../nbs/01.a2l.ipynb 26
class Measurement(Record):
	"""Measurement object like speed,  acc pedal position, etc; a2l section ["PROJECT"]["MODULE"]["MEAUREMENT"]]""" 
	__CAT = 'MEASUREMENT'
	Record.subclass_registry[__CAT] = 'Measurement'

	def __repr__(self):
		try:
			return f'<{self.__class__.__name__}: {self.Name!r}>'
		except AttributeError:
			return super().__repr__() 
	
	@cached_property
	def data_conversion(self):
		try:
			key = self.__dict__['Conversion']['Value']  # define the key for the axis for future fetch
		except KeyError:
			raise KeyError(f'The key "Conversion" is not found in the calibration description or '
						f'the key "Value" is not found in the "Conversion" dict.')
		cat = 'COMPU_METHOD'
		key = f"{super().subclass_registry[cat]}.{key}"
		return self.__class__.fetch(key)

	@cached_property
	def record_type(self):
		try:
			key = self.__dict__['DataType']['Value']  # define the key for the axis for future fetch
		except KeyError:
			raise KeyError(f'The key "Deposit" is not found in the calibration description or '
						f'the key "Value" is not found in the "Deposit" dict.')
		cat = 'RECORD_LAYOUT'
		key = f"{super().subclass_registry[cat]}.{key}"  # define the key for the axis for future fetch
		# print(key)
		rtype = self.__class__.fetch(key)
		if type(rtype) is str:  # if the key is not found in the registry, then it is a scalar
			key = f"{super().subclass_registry[cat]}.{'Scalar_' + rtype}"  # construct the new key for the scalar as defined in a2l
			rtype = self.__class__.fetch(key)
		
		return rtype

	@cached_property
	def address(self):
		return hex(int(self.__dict__['ECU_ADDRESS']['Address']['Value']))[2:]   # transform Ecu address to hex string without '0x'

# %% ../nbs/01.a2l.ipynb 27
class AxisScale(Record):
	"""Target calibration object for torque map; a2l section ["PROJECT"]["MODULE"]["AXIS_PTS"]""" 
	__CAT = 'AXIS_PTS'
	Record.subclass_registry[__CAT] = 'AxisScale'

	def __repr__(self):
		try:
			return f'<{self.__class__.__name__}: {self.Name!r}>'
		except AttributeError:
			return super().__repr__() 
	
	@cached_property
	def data_conversion(self):
		try:
			key = self.__dict__['Conversion']['Value']  # define the key for the axis for future fetch
		except KeyError:
			raise KeyError(f'The key "Conversion" is not found in the calibration description or '
						f'the key "Value" is not found in the "Conversion" dict.')
		cat = 'COMPU_METHOD'
		key = f"{super().subclass_registry[cat]}.{key}"
		return self.__class__.fetch(key)
	
	@cached_property
	def record_type(self):
		try:
			key = self.__dict__['DepositR']['Value']  # define the key for the axis for future fetch
		except KeyError:
			raise KeyError(f'The key "DepositR" is not found in the calibration description or '
						f'the key "Value" is not found in the "DepositR" dict.')
		cat = 'RECORD_LAYOUT'
		key = f"{super().subclass_registry[cat]}.{key}"  # define the key for the axis for future fetch
		rtype = self.__class__.fetch(key)
		if type(rtype) is str:  # if the key is not found in the registry, then it is a scalar
			key = f"{super().subclass_registry[cat]}.{'Scalar_' + rtype}"  # construct the new key for the scalar as defined in a2l
			rtype = self.__class__.fetch(key)
		# if type(rtype) is str:  # if the key is not found in the registry, then it is a scalar
		# 	key = 'Scalar_' + rtype  # construct the new key for the scalar as defined in a2l
		# 	cat = 'RECORD_LAYOUT'
		# 	rtype = self.__class__.fetch(key)
		
		return rtype

	@cached_property
	def address(self):
		return hex(int(self.__dict__['Address']['Value']))[2:]   # transform Ecu address to hex string without '0x'

	@cached_property
	def input(self):
		try:
			key = self.__dict__['InputQuantity']['Value']  # define the key for the axis for future fetch
		except KeyError:
			raise KeyError(f'The key "Conversion" is not found in the calibration description or '
						f'the key "Value" is not found in the "Conversion" dict.')
		cat = 'MEASUREMENT'
		key = f"{super().subclass_registry[cat]}.{key}"
		return self.__class__.fetch(key)

# %% ../nbs/01.a2l.ipynb 28
class DataConversion(Record):
	"""Data conversion object for calibration; a2l section ["PROJECT"]["MODULE"]["COMPU_METHOD"]]""" 
	__CAT = 'COMPU_METHOD'
	Record.subclass_registry[__CAT] = 'DataConversion'

	def __repr__(self):
		try:
			return f'<{self.__class__.__name__}: {self.Name!r}>'
		except AttributeError:
			return super().__repr__() 

# %% ../nbs/01.a2l.ipynb 29
class DataLayout(Record):
	"""Data type object for calibration; a2l section ["PROJECT"]["MODULE"]["RECORD_LAYOUT"]""" 
	__CAT = 'RECORD_LAYOUT'
	Record.subclass_registry[__CAT] = 'DataLayout'
	# size: int=Field(default=4, description='size of the data in bytes')

	def __repr__(self):
		try:
			return f'<{self.__class__.__name__}: {self.Name!r}>'
		except AttributeError:
			return super().__repr__() 
	

	@property
	def data_type(self):
		try:
			dtype = self.__dict__['FNC_VALUES']['DataType']['Value']  # define the key for the axis for future fetch
		except KeyError:
			try:
				dtype = self.__dict__['AXIS_PTS_X']['DataType']['Value']  # define the key for the axis for future fetch
			except KeyError:
				raise KeyError(f'The key "DataType" is not found in the RECORD_LAYOUT "FNC_VALUES" or "AXIS_PTS_X" section'
							f'or the key "Value" is not found in the "DataType" dict.')
		return dtype

	@cached_property
	def type_size(self):
		match(self.data_type):
			case 'UBYTE' | 'SBYTE' | 'CHAR':
				return 1
			case 'UWORD' | 'SWORD':
				return 2
			case 'ULONG' | 'SLONG' | 'FLOAT32_IEEE':
				return 4
			case 'UINT64' | 'INT64' | 'FLOAT64_IEEE':
			 	return 8
			case _:
				raise ValueError(f'Invalid data type {self.data_type}')

# %% ../nbs/01.a2l.ipynb 31
def load_class_type_a2l_lazy(path: Path, jnode_path: Optional[JsonNodePath]=JsonNodePath('/PROJECT/MODULE[]'))->type(Enum):  # return a class type
	""" Search for the calibration key in the A2L file.
	Descripttion: Load the A2L file as a dictionary.
	
	Create record type (enum class) for the calibration parameter for the given a2l json file

	Args:
		path (str): The path to the A2L file.
		section_key (str): The section key to search for the calibration type.

	Returns:
		dict: The A2L file as a dictionary.
	"""
	record_type_keys = []
	with open(path, "r") as f:
		for object in ijson.items(f, jnode_path.lazy_path):
			# print(list(object.keys()))
			record_type_keys += object.keys()
		# keys = [k for k, v in ijson.kvitems(f, prefix) if type(v) is list]t]
		# record_type_key.append(keys)

	RecordTypes = Enum('RecordType', record_type_keys)
	return RecordTypes

# %% ../nbs/01.a2l.ipynb 33
def load_records_lazy(path: Path, leaves: list[str], jnode_path: Optional[JsonNodePath]=JsonNodePath('/PROJECT/MODULE[]'))->dict[Record]:
	"""load records from a json file lazily

	Args:
		path (Path): path to the json file
		# use ijson no need for  jnode_paths, though sacrificing a little bit efficiency (list[JsonNodePath]): list of JsonNodePath to the leaves
		leaves (list[str]): list of leaf indices to the records, needs to be unique and in the first item of the a2l json file

	Returns:
		dict[str, Record]: dict of Records and its subclasses, indexed by the leaf indices
	"""
	registry = {}
	Record.load_types(path, jnode_path= jnode_path)  # init subclass_registry in  Record
	prefix = '' 
	event = ''
	leaf = ''
	map_array_levels = []
	leaves = list(leaves)  # make a shallow copy, so that no new leaves will be added, only referencs to leaves will be removed 
	# Find the record
	with open(path, "r") as f:
		
		parse_events = ijson.parse(f)
		while leaves:
			while True:
				try:
					prefix, event, value = next(parse_events)
					# print(f'prefix: {prefix}, event: {event}, value: {value}')
					match(event):
						case 'start_map':
							map_array_levels.append('m')
						case 'end_map':
							m = map_array_levels.pop()
							assert m == 'm', f'Invalid map level {m}'
						case 'start_array':
							map_array_levels.append('a')
						case 'end_array':
							a = map_array_levels.pop() 
							assert a == 'a', f'Invalid array level {a}'
						case 'map_key':
							pass
						case 'string':
							if value in leaves \
								and prefix.split('.')[-2] == 'Name' \
								and ''.join(map_array_levels) == 'mmamamm':
								# and map_level==4 \
								# and array_level==1:  # find the leaf with the key "Name", is the index of  an a2l record
								# print(f'prefix: {prefix}, event: {event}, value: {value}, map_array_levels: {"".join(map_array_levels)}')
								leaf = value
								leaves.remove(leaf)
								break  # leaf must be located at the 4th level of the map, and 1st level of the array
						case _:
							continue
				except StopIteration as exc:
					raise ValueError(f'leaves {leaves} not found in {path}') from exc

			# Extract the record, Name shoulb be gone as the first key
			# prefix = ".".join(prefix.split('.')[:-2])  # remove the last two segments "Name" and "Value", return to the root of  the item
			# map_array_levels.pop()  # remove the last level "m", return to the rest of record
		
			prefix, event, value = next(parse_events)  # get the end map event
			current_prefix = ".".join(prefix.split('.')[:-1])  # remove the last segment "Value", return to the root of  the item
			if event != 'end_map':
				raise ValueError(f'Invalid event {event} after the leaf {leaf} is found!')
			else:
				m = map_array_levels.pop()  # remove the last level "m", return to the rest of record
				assert m == 'm', f'Invalid map level {m}'
				peer_level = "".join(map_array_levels)
				ending_level = "".join(map_array_levels)[:-1]
		
			# Init the record and the captured
			record = {}
			record['Name'] = leaf   # add the calibration key as name back to the record
			name = leaf  # init name for the record is the current map key where the leaf is found
			last_event = event
			n = None  # current node
			while True:
				try:
					prefix, event, value = next(parse_events)
					record_path = prefix.replace(current_prefix, "").split('.')
					record_path.pop(0)
					# if record_path[0] == '':  # remove the first empty string
					# record_path.pop(0)
					# print(f'prefix: {record_path}, event: {event}, value: {value}')
					match(event):
						case 'start_map':  # open up a new map on the current nested level
							map_array_levels.append('m')
							# start nested map
							if last_event == 'map_key':  # if last event is map_key, then the current node is a map
								assert record_path[-1]==name, f'Invalid record path {record_path} for {value}!'  # confirm map key is the last part of the prefixeee
								n.update({name:{}})  # add the nested map
								n = n[name]  # get down the nested map 
							elif last_event == 'start_array':  # if last event is array, then the current node is an array
								assert record_path[-1]=='item', f'Invalid record path {record_path} for {value}!'  # confirm map key is the last part of the prefixeee
								n.append({})  # append another map to the array
								n = n[-1]  # move to the newly created last item  in the  nested array
							elif last_event == 'end_map':  # if last event is array, then the current node is an array
								assert record_path[-1]=='item', f'Invalid record path {record_path} for {value}!'  # confirm map key is the last part of the prefixeee
								# have to update n, because the last event is not map_key, where n is updated
								n = record
								for k in record_path[:-1]:
									if k != 'item':
										n = n[k]
									else:  # k == 'item', array  											assert len(d)>0, f'Invalid array {d}'
										assert len(n)>0, f'Invalid array {n}'
										n = n[-1]  # get the last item, when generating, fill the items in order
								n.append({})  # append another map to the array
								n = n[-1]  # move to the newly created last item  in the  nested array
							else:
								raise ValueError(f'{event} should not follow {last_event}!')
						case 'end_map':
							m = map_array_levels.pop()
							assert m == 'm', f'Invalid map level {m}'
							# if last_event == 'start_map':  #  empty map, no value
							# 	# last_value[name]={}
							# 	pass
							name = ''  # reset map key

							if "".join(map_array_levels)==ending_level:  # 'mmama':
								break
							# elif ".".join(map_array_levels)==peer_level:
							# 	record.update(captured)  # absorb the captured into the record
							# 	captured = None  # reset the captured
							# else:  # deeper than peer level
							# 	captured = record

						case 'start_array':
							map_array_levels.append('a')
							assert last_event == 'map_key', f'{event} should not follow {last_event}!'  # confirm you have the key
							assert record_path[-1]==name, f'Invalid record path {record_path} for {value}! Data corrupted!'  # confirm map key is the last part of the prefixeee
							assert n[name]=='', f'Invalid init map {n[name]}!'  # confirm map key is the last part of the prefixeee 
							n.update({name:[]})  # update default nested dict to default nested array
							n = n[name]  # get down the nested array
						case 'end_array':
							a = map_array_levels.pop() 
							assert a == 'a', f'Invalid array level {a}'
							name = ''

							if ".".join(map_array_levels)==ending_level: # if we want to extract an array, then we need to go to the peer level 
								break
						case 'map_key':
							name = value 
							n = record
							for k in record_path:
								if k != 'item':
									n = n[k]
								else:  # k == 'item', array  											assert len(d)>0, f'Invalid array {d}'
									assert len(n)>0, f'Invalid array {n}'
									n = n[-1]  # get the last item, when generating, fill the items in order
							n.update({name:''})  # add the key to the map

						case 'null' |'boolean' | 'integer' | 'double' | 'number' | 'string':
							assert name!='', f'map key not available!'
							if map_array_levels[-1]=='m':   # last_event=''start_map' is volatile!
								n[name]=value  # single key-value pair in the same map 
							else:  # map_array_levels[-1] == 'a':
								n[name].append(value)
						case _:
							raise ValueError(f'{event} should not occur! Data corrupted!')

					last_event = event
				except StopIteration as exc:
					raise ValueError(f'{leaf} data corrupted!') from exc
			# for k,v in ijson.kvitems(parse_events, prefix):
			# 	if k == 'Name': 
			# 		if v['Value'] == leaf:
			# 			break
			# rec = {k:v for k,v in ijson.kvitems(parse_events, prefix)}
		
			record_type = set(re.split(r'\.', prefix)).intersection(set(RecordTypes.__members__.keys()))
			assert len(record_type)==1, f'Invalid record type/s {record_type}'
			category = record_type.pop()
			cls_name = Record.subclass_registry.get(category, 'Record')
			cls = globals().get(cls_name, Record)

			if inspect.isclass(cls) and issubclass(cls, Record):
				factory = cls
			else:
				factory = Record
		
			key = f'{cls_name}.{leaf}'
			registry[key] = factory(**record)  # create the record object and add it to the record registry
			
	return registry

# %% ../nbs/01.a2l.ipynb 46
class XCPConfig(BaseModel):
	"""XCP configuration for the calibration parameter"""
	channel: int = Field(default=3, ge=0, le=10000, description='XCP channel')
	download_can_id: str = Field(default='630', ge='0', alias='download', validate_default=True, description='CAN ID for download')
	upload_can_id: str = Field(default='631', ge='0', alias='upload', validate_default=True, description='CAN ID for upload')


# %% ../nbs/01.a2l.ipynb 48
type_collection =  set(['UBYTE', 'SBYTE', 'CHAR', 'UWORD', 'SWORD', 'ULONG', 'SLONG', 'FLOAT32_IEEE', 'UINT64', 'INT64', 'FLOAT64_IEEE'])

def check_a2l_type(v: str) -> str:
	assert v in type_collection, f'Invalid data type {v}'
	return v

A2LType = Annotated[str, AfterValidator(check_a2l_type)]

# %% ../nbs/01.a2l.ipynb 52
class XCPData(BaseModel):
	"""XCP data for the calibration parameter"""
	name: str = Field(default='TQD_trqTrqSetNormal_MAP_v', description='XCP calibration name')
	address: Optional[str] = Field(default='7000aa2a',pattern=r'^[0-9A-Fa-f]{8}$', description='Target Ecu address')
	dim: conlist(Annotated[int,Field(gt=0,lt=50)],min_length=2,max_length=2)
	value_type: A2LType = Field(default='FLOAT32_IEEE', description='Customized XCP data type')
	value_length: int = Field(default=4,multiple_of=2,gt=0,description='XCP data type length in Bytes')
	value: str = Field(pattern=r'^[0-9A-Fa-f]{0,3000}$', min_length=1, max_length=3000, description='XCP calbiration data')

	@model_validator(mode="after")
	def check_map_dimension(self) -> 'XCPData':
		array_size = self.dim[0]*self.dim[1]*self.type_size
		if len(self.value)!=array_size*2:
			raise ValueError(f'value length {len(self.value)}!=(dimension {self.dim})*(value length {array_size*2})!')
		return self
	
	@model_validator(mode="after")
	def check_value_length(self) -> 'XCPData':
		if self.value_length!=self.type_size:
			raise ValueError(f'Value length {self.value_length} doesn\'t match data type {self.value_type}({self.type_size})!')
		return self

	@cached_property
	def type_size(self):
		match(self.value_type):
			case 'UBYTE' | 'SBYTE' | 'CHAR':
				return 1
			case 'UWORD' | 'SWORD':
				return 2
			case 'ULONG' | 'SLONG' | 'FLOAT32_IEEE':
				return 4
			case 'UINT64' | 'INT64' | 'FLOAT64_IEEE':
				return 8
			case _:
				raise ValueError(f'Invalid data type {self.value_type}')

	def __repr__(self) -> str:
		
		return pformat(self.__dict__)

# %% ../nbs/01.a2l.ipynb 53
def Get_Init_XCPData(path: Path=Path('../res/init_value_17rows.json'))->List[XCPData]:

	xcp_data = []
	with open(path) as f:   
		init_values = json.load(f)
		data = init_values['data']
		for v in data:
			try:
				xcp_data.append(XCPData(**v))
			except ValidationError as exc:
				print(exc)
	
	return xcp_data

# %% ../nbs/01.a2l.ipynb 57
class XCPCalib(BaseModel):
	"""XCP calibration parameter"""
	config: XCPConfig = Field(default_factory=XCPConfig, description='XCP configuration')
	data: List[XCPData] = Field(default_factory=List[XCPData], description='list of XCP calibration data')

	# @model_serializer
	# def ser_model(self):
	# 	data = [d.model_dump() for d in self.data]
	# 	res = self.config.model_dump(), 
	# 	res.update({'data': data})
		# return res

# %% ../nbs/01.a2l.ipynb 58
def Get_XCPCalib_From_XCP(path: Path=Path('../res/download.json'))->List[XCPData]:

	with open(path) as f:   
		d = json.load(f)
		data = d['data']
		xcp_data = []
		for v in data:
			xcp_data.append(XCPData(**v))

		config =d['config']
		xcp_config = XCPConfig(config=config)
	
	xcp_calib = XCPCalib(config=xcp_config, data=xcp_data)
	
	return xcp_calib

# %% ../nbs/01.a2l.ipynb 59
def Generate_XCPData(
		a2l: Path=Path('../res/vbu_sample.json'), 
		keys: List[str]=['TQD_trqTrqSetNormal_MAP_v',
					"VBU_L045A_CWP_05_09T_AImode_CM_single",
					"Lookup2D_FLOAT32_IEEE",
					"Lookup2D_X_FLOAT32_IEEE, " 
					"TQD_vVehSpd",
					"TQD_vSgndSpd_MAP_y",
					"TQD_pctAccPedPosFlt",
					"TQD_pctAccPdl_MAP_x"],
		node_path: str='/PROJECT/MODULE[]',
		default_xcpdata: str=2856*'0')->XCPCalib:
	"""Generate XCP calibration parameter from A2L file and calibration parameter name

	Args:
		a2l (Path): path to the A2L file
		calib (str): calibration parameter name

	Returns:
		XCPCalib: XCP calibration parameter
	"""

	# load calibration parameter from A2L file
	calibs = load_records_lazy(a2l, keys, JsonNodePath(node_path))
	idx = 'Calibration.' + keys[0]
	calib = calibs[idx]

	# create XCP calibration parameter
	xcp_data = XCPData(name=calib.Name, 
					address=calib.address, 
					dim=[calib.axes[0].MaxAxisPoints['Value'],
						calib.axes[1].MaxAxisPoints['Value']], 
					value_type=calib.record_type.data_type,
					value_length=calib.record_type.type_size,
					value=default_xcpdata)

	return xcp_data

# %% ../nbs/01.a2l.ipynb 70
def load_a2l_lazy(path: Path, leaves: list[str])->dict:
	""" Search for the calibration key in the A2L file.
	Descripttion: Load the A2L file as a dictionary.

	Args:
		path (str): The path to the A2L file.
		calib_key (str): The node path to the calibration parameters.

	Returns:
		dict: The A2L file as a dictionary.
	"""
	records = []
	for leaf in leaves:
		prefix = ''
		with open(path, "r") as f:
			parser = ijson.parse(f)
			while True:
				prefix, event, value = next(parser)
				if value == leaf:
					break
			else:
				raise ValueError(f'Key {key} not found in the A2L file.')
			
			prefix = ".".join(prefix.split('.')[:-2])  # remove the last two segments "Name" and "Value", return to the root of  the item
			objects =  ijson.kvitems(parser, prefix)
			record  = {k:v for k,v in objects}
			record['Name'] = leaf   # add the calibration key as name back to the record
			records.append(record)

	return records

# %% ../nbs/01.a2l.ipynb 72
def load_a2l_eager(path: Path, jnode_path: JsonNodePath=JsonNodePath('/PROJECT/MODULE[]'))->dict:
	""" Load the A2L file as a dictionary.
	Descripttion: Load the A2L file as a dictionary.

	Args:
		path (Path): The path to the A2L file.
		node (str): The node to search for, e.g. "/PROJECT/MODULE[0]/CHARACTERISTIC".

	Returns:
		dict: The A2L file as a dictionary.
	"""
	records = {}
	path_list = re.split(r'\.', jnode_path.lazy_path)[:-1]
	with open(path, "r") as f:
		n = json.load(f)
		for p in path_list:
			n = n[p] 
		# only the first module is used
		# sections = ['CHARACTERISTIC', 'MEASUREMENT', 'AXIS_PTS', 'COMPU_METHOD']
		# for s in sections:
		# 	for key, value in n[s].items():
		# 		print(key, value)
				# if prefix.endswith(".ECU_ADDRESS"):
				# 	yield value
			# a2l = json.load(f)
	return n
